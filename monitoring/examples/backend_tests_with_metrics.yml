# Example: Enhanced backend_tests.yml with Grafana metrics export
# Replace your existing backend_tests.yml with this version to enable metrics

name: Backend

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      run_coverage:
        description: 'Run test coverage report'
        required: false
        default: false
        type: boolean

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_savenbite_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    defaults:
      run:
        working-directory: save-n-bite-backend

    steps:
    - uses: actions/checkout@v4

    # Record workflow start time
    - name: Record workflow start time
      id: workflow_start
      run: echo "start_time=$(date +%s)" >> $GITHUB_OUTPUT

    - name: Ensure static and media directories exist
      run: |
        mkdir -p static
        mkdir -p media

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install Poetry
      run: |
        curl -sSL https://install.python-poetry.org | python3 -
        echo "$HOME/.local/bin" >> $GITHUB_PATH

    - name: Cache Poetry dependencies
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pypoetry
          save-n-bite-backend/.venv
        key: ${{ runner.os }}-poetry-${{ hashFiles('save-n-bite-backend/poetry.lock') }}
        restore-keys: |
          ${{ runner.os }}-poetry-

    - name: Configure Poetry
      run: |
        poetry config virtualenvs.create true
        poetry config virtualenvs.in-project true

    - name: Install dependencies with Poetry
      run: |
        poetry install --with dev
        poetry run pip install model_bakery coverage pytest-html

    - name: Install PostgreSQL client
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client

    - name: Create test environment file
      run: |
        cat > .env << EOF
        DEBUG=True
        SECRET_KEY=test-secret-key-for-github-actions-very-long-and-secure
        DB_NAME=test_savenbite_db
        DB_USER=postgres
        DB_PASSWORD=postgres
        DB_HOST=127.0.0.1
        DB_PORT=5432
        REDIS_URL=redis://localhost:6379/0
        ALLOWED_HOSTS=localhost,127.0.0.1
        CORS_ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
        USE_AZURE_STORAGE=False
        TESTING=True
        GEOCODING_ENABLED=False
        AZURE_STORAGE_CONNECTION_STRING=UseDevelopmentStorage=true
        AZURE_CONTAINER_NAME=test-container
        EOF

    - name: Wait for services
      run: |
        echo "Waiting for PostgreSQL..."
        until pg_isready -h 127.0.0.1 -p 5432 -U postgres; do
          echo "PostgreSQL not ready, waiting..."
          sleep 2
        done
        echo "âœ… PostgreSQL is ready"

    - name: Run migrations
      run: poetry run python manage.py migrate

    - name: Run Django system check
      run: poetry run python manage.py check

    # Run tests with metrics collection
    - name: Run analytics app tests
      id: test_analytics
      continue-on-error: true
      run: |
        start=$(date +%s)
        poetry run python manage.py test analytics --verbosity=2 > test_output.txt 2>&1
        exit_code=$?
        end=$(date +%s)
        duration=$((end - start))
        
        # Parse test results
        if [ $exit_code -eq 0 ]; then
          echo "status=success" >> $GITHUB_OUTPUT
          grep -oP 'Ran \K\d+' test_output.txt | head -1 > total.txt || echo "0" > total.txt
          echo "total=$(cat total.txt)" >> $GITHUB_OUTPUT
          echo "passed=$(cat total.txt)" >> $GITHUB_OUTPUT
          echo "failed=0" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          grep -oP 'Ran \K\d+' test_output.txt | head -1 > total.txt || echo "0" > total.txt
          grep -oP 'FAILED.*\(failures=\K\d+' test_output.txt | head -1 > failed.txt || echo "1" > failed.txt
          echo "total=$(cat total.txt)" >> $GITHUB_OUTPUT
          echo "failed=$(cat failed.txt)" >> $GITHUB_OUTPUT
          passed=$(($(cat total.txt) - $(cat failed.txt)))
          echo "passed=$passed" >> $GITHUB_OUTPUT
        fi
        echo "duration=$duration" >> $GITHUB_OUTPUT
        cat test_output.txt
        exit $exit_code

    - name: Run authentication app tests
      id: test_auth
      continue-on-error: true
      run: |
        start=$(date +%s)
        poetry run python manage.py test authentication --verbosity=2 > test_output.txt 2>&1
        exit_code=$?
        end=$(date +%s)
        duration=$((end - start))
        
        if [ $exit_code -eq 0 ]; then
          echo "status=success" >> $GITHUB_OUTPUT
          grep -oP 'Ran \K\d+' test_output.txt | head -1 > total.txt || echo "0" > total.txt
          echo "total=$(cat total.txt)" >> $GITHUB_OUTPUT
          echo "passed=$(cat total.txt)" >> $GITHUB_OUTPUT
          echo "failed=0" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          grep -oP 'Ran \K\d+' test_output.txt | head -1 > total.txt || echo "0" > total.txt
          grep -oP 'FAILED.*\(failures=\K\d+' test_output.txt | head -1 > failed.txt || echo "1" > failed.txt
          echo "total=$(cat total.txt)" >> $GITHUB_OUTPUT
          echo "failed=$(cat failed.txt)" >> $GITHUB_OUTPUT
          passed=$(($(cat total.txt) - $(cat failed.txt)))
          echo "passed=$passed" >> $GITHUB_OUTPUT
        fi
        echo "duration=$duration" >> $GITHUB_OUTPUT
        cat test_output.txt
        exit $exit_code

    # Calculate workflow metrics
    - name: Calculate workflow metrics
      if: always()
      id: workflow_metrics
      run: |
        end_time=$(date +%s)
        start_time=${{ steps.workflow_start.outputs.start_time }}
        duration=$((end_time - start_time))
        echo "duration=$duration" >> $GITHUB_OUTPUT
        
        # Determine overall status
        if [[ "${{ steps.test_analytics.outputs.status }}" == "success" && \
              "${{ steps.test_auth.outputs.status }}" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
        fi

    # Export metrics to Pushgateway (if configured)
    - name: Export workflow metrics
      if: always() && vars.PUSHGATEWAY_URL != ''
      continue-on-error: true
      env:
        PUSHGATEWAY_URL: ${{ vars.PUSHGATEWAY_URL }}
      run: |
        # Install requests if not available
        pip install requests > /dev/null 2>&1 || true
        
        cd $GITHUB_WORKSPACE/monitoring/scripts
        
        # Export workflow metrics
        python3 export_metrics.py workflow \
          "${{ steps.workflow_metrics.outputs.status }}" \
          "${{ steps.workflow_metrics.outputs.duration }}" || true
        
        # Export test metrics for analytics
        if [ -n "${{ steps.test_analytics.outputs.total }}" ]; then
          python3 export_metrics.py test \
            "${{ steps.test_analytics.outputs.total }}" \
            "${{ steps.test_analytics.outputs.passed }}" \
            "${{ steps.test_analytics.outputs.failed }}" \
            "0" \
            "${{ steps.test_analytics.outputs.duration }}" \
            --app "analytics" || true
        fi
        
        # Export test metrics for authentication
        if [ -n "${{ steps.test_auth.outputs.total }}" ]; then
          python3 export_metrics.py test \
            "${{ steps.test_auth.outputs.total }}" \
            "${{ steps.test_auth.outputs.passed }}" \
            "${{ steps.test_auth.outputs.failed }}" \
            "0" \
            "${{ steps.test_auth.outputs.duration }}" \
            --app "authentication" || true
        fi

    - name: Run coverage report (if requested)
      if: ${{ github.event.inputs.run_coverage == 'true' || github.event_name == 'pull_request' }}
      id: coverage
      run: |
        poetry run pytest --cov=. --cov-report=html --cov-report=term-missing --cov-report=json
        
        # Extract coverage percentage
        coverage_percent=$(python3 -c "import json; data=json.load(open('coverage.json')); print(data['totals']['percent_covered'])")
        echo "percent=$coverage_percent" >> $GITHUB_OUTPUT

    - name: Export coverage metrics
      if: always() && steps.coverage.outputs.percent != '' && vars.PUSHGATEWAY_URL != ''
      continue-on-error: true
      env:
        PUSHGATEWAY_URL: ${{ vars.PUSHGATEWAY_URL }}
      run: |
        cd $GITHUB_WORKSPACE/monitoring/scripts
        python3 export_metrics.py coverage \
          "${{ steps.coverage.outputs.percent }}" \
          --app "backend" || true

    - name: Upload coverage reports
      if: ${{ github.event.inputs.run_coverage == 'true' || github.event_name == 'pull_request' }}
      uses: actions/upload-artifact@v4
      with:
        name: coverage-report
        path: save-n-bite-backend/htmlcov/

    # Fail the job if any tests failed
    - name: Check test results
      if: steps.test_analytics.outputs.status == 'failure' || steps.test_auth.outputs.status == 'failure'
      run: exit 1
